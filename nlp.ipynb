{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       "-1.9254  0.2848  0.2735  0.1440  1.5715  1.2846 -1.3029 -0.3067  0.0190 -2.1332\n",
       "-0.6516 -1.9692 -0.8449  0.1270  1.1009  1.3408  1.8494 -0.8904 -1.3814  0.1601\n",
       " 0.9403  0.1200 -2.7859  0.6495 -2.1209  0.7630 -0.4578  0.0622 -0.6080 -1.3222\n",
       "-0.6516 -1.9692 -0.8449  0.1270  1.1009  1.3408  1.8494 -0.8904 -1.3814  0.1601\n",
       " 0.0884  0.7957  1.3944  0.3304  1.2315 -0.5400  1.2227  0.0030  1.0206  2.1087\n",
       "\n",
       "Columns 10 to 19 \n",
       "-1.2497 -1.8067 -0.4380 -0.4508 -0.2988 -1.0211  1.0691 -0.2588 -0.5100 -1.4161\n",
       "-1.8485  2.1402  0.5949 -0.3385 -0.2733 -0.2851  0.0870 -1.7878 -0.9565 -0.3677\n",
       "-0.3339  0.6801 -0.6570  0.8762 -0.3547 -0.5849 -0.6250  0.2425 -0.6209 -1.7259\n",
       "-1.8485  2.1402  0.5949 -0.3385 -0.2733 -0.2851  0.0870 -1.7878 -0.9565 -0.3677\n",
       " 1.8358 -0.0632  0.1345  0.3971 -1.6751 -0.2942 -1.6081 -0.4600 -0.6743  1.7126\n",
       "[torch.FloatTensor of size 5x20]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable as V\n",
    "\n",
    "\n",
    "emb = nn.Embedding(10000, 20, padding_idx=0)\n",
    "inp = V(torch.LongTensor([1, 2, 5, 2, 10]))\n",
    "out = emb(inp)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "import glob\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "\n",
    "remove_marks_regex = re.compile('[,\\.\\(\\)\\[\\]\\*:;]|<.*?>')\n",
    "shift_marks_regex = re.compile('([?!])')\n",
    "\n",
    "def text2ids(text, vacab_dict):\n",
    "    text = remove_marks_regex.sub('', text)\n",
    "    text = shift_marks_regex.sub(r' \\l ', text)\n",
    "    tokens = text.split()\n",
    "    return [vacab_dict.get(token, 0) for token in tokens]\n",
    "\n",
    "def list2tensor(token_idxes, max_len=100, padding=True):\n",
    "    if len(token_idxes) > max_len:\n",
    "        token_idxes = token_idxes[:max_len]\n",
    "    n_tokens = len(token_idxes)\n",
    "    if padding:\n",
    "        token_idxes = token_idxes + [0] * (max_len - len(token_idxes))\n",
    "    return torch.LongTensor(token_idxes), n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dir_path, train=True, max_len=100, padding=True):\n",
    "        self.max_len = max_len\n",
    "        self.padding = padding\n",
    "        path = pathlib.Path(dir_path)\n",
    "        vocab_path = path.joinpath('imdb.vocab')\n",
    "        self.vocab_array = vocab_path.open().read().strip().splitlines()\n",
    "        self.vocab_dict = dict((w, i+1) for (i, w) in enumerate(self.vocab_array))\n",
    "        if train:\n",
    "            target_path = path.joinpath('train')\n",
    "        else:\n",
    "            target_path = path.joinpath('test')\n",
    "        pos_files = sorted(glob.glob(str(target_path.joinpath('pos/*.txt'))))\n",
    "        neg_files = sorted(glob.glob(str(target_path.joinpath('neg/*.txt'))))\n",
    "        self.labeled_files = list(zip([0]*len(neg_files), neg_files)) + list(zip([1]*len(pos_files), pos_files))\n",
    "        \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab_array)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labeled_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label, f = self.labeled_files[idx]\n",
    "        data = open(f).read().lower()\n",
    "        data = text2ids(data, self.vocab_dict)\n",
    "        data, n = list2tensor(data, self.max_len, self.padding)\n",
    "        return data, label, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_data = IMDBDataset('./aclImdb/')\n",
    "train_data = IMDBDataset('./aclImdb/', train=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
