{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 1.3984  0.5117 -1.2055  0.5170  0.1951  0.6920  1.2170  0.0148 -1.3198 -0.0557\n",
       " 0.7494 -0.2517 -0.6541  1.8658  0.7754 -0.1279 -0.8152  0.6345  1.6759 -0.8899\n",
       " 0.8958 -1.0117 -0.0512  0.9927  0.3822  0.0791  0.8742 -1.0904 -0.7844  2.8853\n",
       " 0.7494 -0.2517 -0.6541  1.8658  0.7754 -0.1279 -0.8152  0.6345  1.6759 -0.8899\n",
       "-0.4250 -0.7694  1.0069  0.2646 -1.1900  0.4994 -0.2357 -0.7308  1.9614 -0.0608\n",
       "\n",
       "Columns 10 to 19 \n",
       "-0.6374  0.9245 -1.1659  0.7989  0.4804 -0.8946  0.1104  0.4608  0.1480 -1.9502\n",
       " 0.6689 -0.2627 -0.5234 -1.7893 -1.6666  0.4311  1.2278  0.2549 -0.6590  0.5883\n",
       "-0.7339 -1.1809 -0.7005 -0.2419 -0.9851 -0.1792  0.7095  0.6903  1.5535  1.3207\n",
       " 0.6689 -0.2627 -0.5234 -1.7893 -1.6666  0.4311  1.2278  0.2549 -0.6590  0.5883\n",
       " 0.6197  0.1856  0.2308 -0.6874 -0.0595  0.2519 -2.3235  1.3661  0.3801 -0.3416\n",
       "[torch.FloatTensor of size 5x20]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable as V\n",
    "\n",
    "\n",
    "emb = nn.Embedding(10000, 20, padding_idx=0)\n",
    "inp = V(torch.LongTensor([1, 2, 5, 2, 10]))\n",
    "out = emb(inp)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "import glob\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "\n",
    "remove_marks_regex = re.compile('[,\\.\\(\\)\\[\\]\\*:;]|<.*?>')\n",
    "shift_marks_regex = re.compile('([?!])')\n",
    "\n",
    "def text2ids(text, vacab_dict):\n",
    "    text = remove_marks_regex.sub('', text)\n",
    "    text = shift_marks_regex.sub(r' \\1 ', text)\n",
    "    tokens = text.split()\n",
    "    return [vacab_dict.get(token, 0) for token in tokens]\n",
    "\n",
    "def list2tensor(token_idxes, max_len=100, padding=True):\n",
    "    if len(token_idxes) > max_len:\n",
    "        token_idxes = token_idxes[:max_len]\n",
    "    n_tokens = len(token_idxes)\n",
    "    if padding:\n",
    "        token_idxes = token_idxes + [0] * (max_len - len(token_idxes))\n",
    "    return torch.LongTensor(token_idxes), n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dir_path, train=True, max_len=100, padding=True):\n",
    "        self.max_len = max_len\n",
    "        self.padding = padding\n",
    "        path = pathlib.Path(dir_path)\n",
    "        vocab_path = path.joinpath('imdb.vocab')\n",
    "        self.vocab_array = vocab_path.open().read().strip().splitlines()\n",
    "        self.vocab_dict = dict((w, i+1) for (i, w) in enumerate(self.vocab_array))\n",
    "        if train:\n",
    "            target_path = path.joinpath('train')\n",
    "        else:\n",
    "            target_path = path.joinpath('test')\n",
    "        pos_files = sorted(glob.glob(str(target_path.joinpath('pos/*.txt'))))\n",
    "        neg_files = sorted(glob.glob(str(target_path.joinpath('neg/*.txt'))))\n",
    "        self.labeled_files = list(zip([0]*len(neg_files), neg_files)) + list(zip([1]*len(pos_files), pos_files))\n",
    "        \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab_array)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labeled_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label, f = self.labeled_files[idx]\n",
    "        data = open(f).read().lower()\n",
    "        data = text2ids(data, self.vocab_dict)\n",
    "        data, n = list2tensor(data, self.max_len, self.padding)\n",
    "        return data, label, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_data = IMDBDataset('./aclImdb/')\n",
    "test_data = IMDBDataset('./aclImdb/', train=False)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTaggingNet(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim=50, hidden_size=50, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x, h0=None, l=None):\n",
    "        x = self.emb(x)\n",
    "        x, h = self.lstm(x, h0)\n",
    "        if l is not None:\n",
    "            x = x[list(range(len(x))), l-1, :]\n",
    "        else:\n",
    "            x = x[:, -1, :]\n",
    "        x = self.linear(x)\n",
    "        x = x.squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_net(net, data_loader):\n",
    "    net.eval()\n",
    "    ys = []\n",
    "    ypreds = []\n",
    "    for x, y, l in data_loader:\n",
    "        x = V(x, volatile=True)\n",
    "        y = V(y, volatile=True)\n",
    "        y_pred = net(x, l=l)\n",
    "        y_pred = (y_pred > 0).long()\n",
    "        ys.append(y.data)\n",
    "        ypreds.append(y_pred.data)\n",
    "    ys = torch.cat(ys)\n",
    "    ypreds = torch.cat(ypreds)\n",
    "    acc = (ys == ypreds).float().sum() / len(ys)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6599768895627288 0.7222 0.6944\n",
      "1 0.5338538296311103 0.79404 0.7454\n",
      "2 0.4424481616948572 0.83256 0.76496\n",
      "3 0.3784253418902912 0.86392 0.77488\n",
      "4 0.31670292347783935 0.89372 0.78396\n",
      "5 0.26823607367723035 0.91796 0.7894\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "net = SequenceTaggingNet(train_data.vocab_size+1, num_layers=2)\n",
    "opt = optim.Adam(net.parameters())\n",
    "loss_f = nn.BCEWithLogitsLoss()\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "    net.train()\n",
    "    for x, y, l in train_loader:\n",
    "        x = V(x)\n",
    "        y = V(y.float()).float()\n",
    "        y_pred = net(x, l=l)\n",
    "        loss = loss_f(y_pred, y)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.data[0])\n",
    "    train_acc = eval_net(net, train_loader)\n",
    "    val_acc = eval_net(net, test_loader)\n",
    "    print(epoch, mean(losses), train_acc, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.89876, 0.39608)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "train_X, train_y = load_svmlight_file('./aclImdb/train/labeledBow.feat')\n",
    "test_X, test_y = load_svmlight_file('./aclImdb/test/labeledBow.feat', n_features=train_X.shape[1])\n",
    "model = LogisticRegression(C=0.1, max_iter=1000)\n",
    "model.fit(train_X, train_y)\n",
    "model.score(train_X, train_y), model.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTaggingNet(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim=50, hidden_size=50, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x, h0=None, l=None):\n",
    "        x = self.emb(x)\n",
    "        if l is not None:\n",
    "            x = nn.utils.rnn.pack_padded_sequence(x, l, batch_first=True)\n",
    "        x, h = self.lstm(x, h0)\n",
    "        if l is not None:\n",
    "            hidden_state, cell_state = h\n",
    "            x = hidden_state[-1]\n",
    "        else:\n",
    "            x = x[:, -1, :]\n",
    "        x = self.linear(x).squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    losses = []\n",
    "    net.train()\n",
    "    for x, y, l in train_loader:\n",
    "        l, sort_idx = torch.sort(l, descending=True)\n",
    "        x = x[sort_idx]\n",
    "        y = y[sort_idx]\n",
    "        x = V(x)\n",
    "        y = V(y.float())\n",
    "        y_pred = net(x, l=list(l))\n",
    "        loss = loss_f(y_pred, y)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.data[0])\n",
    "    train_acc = eval_net(net, train_loader)\n",
    "    val_acc = eval_net(net, test_loader)\n",
    "    print(epoch, mean(losses), train_acc, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "all_chars = string.printable\n",
    "vocab_size = len(all_chars)\n",
    "vocab_dict = dict((c, i) for (i, c) in enumerate(all_chars))\n",
    "\n",
    "def str2ints(s, vocab_dict):\n",
    "    return [vocab_dict[c] for c in s]\n",
    "\n",
    "def ints2str(x, vocab_array):\n",
    "    return \"\".join([vocab_array[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt > tinyshakespeare.txt\n",
    "\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, path, chunk_size=200):\n",
    "        data = str2ints(open(path).read().strip(), vocab_dict)\n",
    "        data = torch.LongTensor(data).split(chunk_size)\n",
    "        if len(data[-1]) < chunk_size:\n",
    "            data = data[:-1]\n",
    "        self.data = data\n",
    "        self.n_chunks = len(self.data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_chunks\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ShakespeareDataset('./tinyshakespeare.txt', chunk_size=200)\n",
    "loader = DataLoader(ds, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceGenerationNet(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim=50, hidden_size=50, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, num_embeddings)\n",
    "        \n",
    "    def forward(self, x, h0=None):\n",
    "        x = self.emb(x)\n",
    "        x, h = self.lstm(x, h0)\n",
    "        x = self.linear(x)\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(net, start_phrase='The King said', length=200, temperature=0.8):\n",
    "    net.eval()\n",
    "    result = []\n",
    "    start_tensor = torch.LongTensor(str2ints(start_phrase, vocab_dict))\n",
    "    x0 = V(start_tensor.unsqueeze(0), volatile=True)\n",
    "    o, h = net(x0)\n",
    "    out_dist = o[:, -1].data.view(-1).exp()\n",
    "    top_i = torch.multinomial(out_dist, 1)[0]\n",
    "    result.append(top_i)\n",
    "    for i in range(length):\n",
    "        inp = torch.LongTensor([[top_i]])\n",
    "        o, h = net(V(inp), h)\n",
    "        out_dist = o.data.view(-1).exp()\n",
    "        top_i = torch.multinomial(out_dist, 1)[0]\n",
    "        result.append(top_i)\n",
    "    return start_phrase + ints2str(result, all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.494255264827183\n",
      "The King saidecCy sErntTtAthrnmTmoel\n",
      "bdGsuoetSS.este,sur:orot \n",
      "tt\n",
      "aghaea ,tkobbnb\n",
      "v.\n",
      "dEfM ut \n",
      " \n",
      "Iuec anns:y mdcu\n",
      "Tsga\n",
      "hut suwounRloeto .iboldbartyers aoaEcmg of iusmn pniha C,tu\n",
      "c mrtadt hua  dtvtm. m Ithoed ueitse\n",
      "1 3.0875517668042862\n",
      "The King saidn\n",
      "Hor tem, aee bun ake hy tho\n",
      "laigrCte\n",
      "\n",
      ": saoa edt bote.\n",
      "Thr,\n",
      "M ban l sne; pes~on eeiy.Tufunr urhmef foeeE,ro tocrd teo wegtt tu'dr rkelrem\n",
      "shileh amhauusha\n",
      "ehik aleed lctesrmee bo dlhah oert:\n",
      "Ath ewat\n",
      "2 2.702447132383074\n",
      "The King saidsd'tet;8s I\n",
      "Nrrhioaav ihs wo. huurut'g wherse gicik Airate eeetedy a the mef eind\n",
      "Hhto yhod wair, iee the, sfoit;\n",
      "No haf ymlle hy shaue. ledrlestr !,f mons sihe fary't\n",
      "\n",
      "ECTAATBTSRD:\n",
      "Q\n",
      "Roed\n",
      "To sad, gudy\n",
      "3 2.489848403930664\n",
      "The King saidce Pnshegal co ersibl hwe taed for tis diyiy gon;\n",
      "matend art, her malk werted dusser ans, eiyech Khee wht ardise thulm conpwhanl ceipod the oom, wocve b nec,\n",
      "Nhfit on:\n",
      "RuP eis, thesud eody\n",
      "hi; lhif sav\n",
      "4 2.3617121750967844\n",
      "The King said souing oy it I shabuisTis lorn to ba, tiy sragh yiy werll befwek anl,\n",
      "Sivh mire you nfeund hond laung gepmeres anl ilgn taar, petath te.\n",
      "Ford,\n",
      "\n",
      "WILMGUI\n",
      "Thind thre le xaru?\n",
      "\n",
      "OBGMIMTG IUNH:\n",
      "Mopertour ye\n",
      "5 2.2727265085492814\n",
      "The King said Indlapt entpore.-\n",
      "\n",
      "GORHY RIAA:\n",
      "T-yon o ess.\n",
      "\n",
      "GAN: AEAhT\n",
      "Lis, 'tifcyime the hom oosd thaunt thicgsy dris:\n",
      "Gwas,\n",
      "Co the to me wacsy heltith me is worgs, dkin the thap hwer nuss, cowanytor?\n",
      "\n",
      "MKTEUYB:\n",
      "To \n",
      "6 2.20732476915632\n",
      "The King said hale-anf usgiad buor mite: kneast 'cod,\n",
      "Cpisher out in. Made?\n",
      "\n",
      "AUNONENGE:\n",
      "Thit lome\n",
      "To me sures ong comy of hol.\n",
      "\n",
      " ATLAHRH:\n",
      "The\n",
      "hole kyele a fary, sof fave lale it wangi thae,\n",
      "In.\n",
      "\n",
      "NINSLAUS:\n",
      "Y brather\n",
      "7 2.1535607733045308\n",
      "The King said wisaly she lokher, sdeint at swresinmed,\n",
      "Woath wish nest is yeon thid avad,\n",
      "fis the louds; mim she my van Myy nee to the hand a it, sue to ay pefiss to Biilb,\n",
      "Ban.\n",
      "Yur, in myoeld im on anzerxteldens,\n",
      "\n",
      "8 2.106196835381644\n",
      "The King said sand ILly than, we of so larnwfito.\n",
      "\n",
      "UOS INGAR:\n",
      "Mo have, bibnus theanse helsce dy yurbency;\n",
      "And vorf undon tasshem nake,\n",
      "You catheres!\n",
      "\n",
      "CINIRY:\n",
      "Apeal. \n",
      "Thoo do pepthy the you in tighest be p an shore \n",
      "9 2.0659859439304897\n",
      "The King said helfocd suith wilvell wige the thes crecroud your,\n",
      "To me shiver, I thail! Beak?\n",
      "If?\n",
      "In me show dever her the baef that loittase,\n",
      "Whoum word of ond's il the;\n",
      "Thas and\n",
      "Thein yourtith inair to, lovus lam\n",
      "10 2.030454443522862\n",
      "The King said if apwing't ceen'd? I moner; thingour scive. Arle, grish! Brood momh comathelt, on themer thing Whave ir in thamsdung dound the kret\n",
      "Hy hrave anly hichoum to nound shus arsticwind,\n",
      "An have with thy so\n",
      "11 1.9989476224354334\n",
      "The King said calloun.\n",
      "\n",
      "RENIUS:\n",
      "Sulchy be toot roveled\n",
      "Guld is vinthay paake suve the, Gobdufort faet on mang, prarkevelk you the maek's?\n",
      "Ra weot am sirs late toulg copengson and of thereet nouf the and and core.\n",
      "\n",
      "\n",
      "12 1.9716064187458584\n",
      "The King said\n",
      "Beal packrother: you your\n",
      "do materes. But arfed sin thmat hiegs cad!\n",
      "And plooy not pumae here, shemer goth,\n",
      "He mocn onser.\n",
      "\n",
      "ESETUTESS VIDAR:\n",
      "Cothice: a the knout aded in,\n",
      "And outshinted to ax liy and \n",
      "13 1.9470952074868337\n",
      "The King said.\n",
      " to mave unsexly therill-'cour thus the pEil'd a a thees whe hithee spunce?\n",
      "A 'magoe unece snot, ead and soot\n",
      "Wemevent, I thefrerow this prowner sait hiw in to worrers\n",
      "To your the such.\n",
      "And her, you \n",
      "14 1.9253773314612252\n",
      "The King said with wilf the canver to he ery;\n",
      "My the fall yow, yirhed of youm mowh thesnius'-Pard, mus so prising,\n",
      "Their this pream, pee,\n",
      "Of pinds pinesinps.\n",
      "\n",
      "AFODHAN:\n",
      "I stich carter.\n",
      "\n",
      "ROMEO:\n",
      "Than with to grlace hi\n",
      "15 1.9064228275844028\n",
      "The King said,'\n",
      "O Lir, you might with my the thasimy it are coreford,\n",
      "Beel powseaghito-tuch are nethend what uenfall-creasuch; pint thear.\n",
      "\n",
      "Mliungred:\n",
      "If thee him, and: be that remin's done carcere all 'fleat'd is \n",
      "16 1.8875773927143642\n",
      "The King said lord ofled all cateljases-\n",
      "Frbelorring so dispoe indesoed't Wad nave to surpmarl\n",
      "A:\n",
      "In I dremand! my, is on bented.\n",
      "\n",
      "BETRYOR OF RERONG:\n",
      "I than fick to short,\n",
      "For and dotteln me! my have-y but ceand.\n",
      "\n",
      "\n",
      "17 1.8705517203467232\n",
      "The King saids lek cremand's by ipere in sees-\n",
      "Bu giict, with the thee.\n",
      "\n",
      "ROGLUTE OF ENXNCE:\n",
      "Whis his by to hake's lifa swall abladsed and cifort.\n",
      "\n",
      "LERIEN:\n",
      "I the lech roight: my, affeeder his coupeiting:\n",
      "Ant revent \n",
      "18 1.854714983531407\n",
      "The King saids\n",
      "To best comlay! Cacarloone toobomte? Whands not the tich hest.\n",
      "\n",
      "CLictARDI:\n",
      "Leve mai me gike are! Seat your, the myartince cad.\n",
      "\n",
      "HODTERSHBRA:\n",
      "Why man, knothin.\n",
      "\n",
      "Pite:\n",
      "They shand; I'll sue to our?\n",
      "\n",
      "PET\n",
      "19 1.8411611352648054\n",
      "The King said all or omere you wettpor\n",
      "But neute teed, at streccoustns my becupn.\n",
      "Thou yat nom we were jelen gigpan; verets sees, niber\n",
      "spree than ang and that sears all it!\n",
      "\n",
      "Simsg;\n",
      "Sicted and have such mad.\n",
      "Thune-\n",
      "20 1.8286724076952252\n",
      "The King said.\n",
      "And heem a warpy the frown what ore the heith.\n",
      "\n",
      "LUCIO:\n",
      "Yest not to come and there\n",
      "Over truran erwhinks lither mow of your we nave are thans,\n",
      "Ous the; must like love, God hall were they a fain so\n",
      "None\n",
      "21 1.8179266473225184\n",
      "The King said\n",
      "That thought, fole, wound to lay.\n",
      "\n",
      "Mlatiance:\n",
      "Wase morbles Yold mest is so firreas, the war; I must ore. God;\n",
      "What lemet in Couen of in of consurst of Ligtigys.\n",
      "Now cutnibure and laves you kive one?\n",
      "H\n",
      "22 1.8063337768827166\n",
      "The King said,\n",
      "Nor, your of fieles a far but, that heavy a, were,\n",
      "Whom to he a by of Ceanstring, to ard;\n",
      "And him solf's, Sarny's so war, the geptray all:\n",
      "And to my oul confukon'd catenly's hat\n",
      "is shuar'd dandle my \n",
      "23 1.7976676177978517\n",
      "The King saidfut lake sent?\n",
      "That bith my are wamlefan, be fold are masty way what thy lot is so no\n",
      "heh frieswer.\n",
      "Whow you proen 'to is betiex thim leath. Send-yout\n",
      "To thou mingrge, my thangibs shourds, bark yours t\n",
      "24 1.7883237321036203\n",
      "The King said offord;\n",
      "Go knat hers;\n",
      "Or is the will.\n",
      "Thet you math, with my powery goth withch.\n",
      "\n",
      "POMKUCHANA:\n",
      "Well wiis. 'tuipblies ax no macterciouch you my suit\n",
      "There seid your wisce for arans, gut are and horder'd\n",
      "25 1.7797843524387904\n",
      "The King said vranonign yough\n",
      "Nowmlyted have and know Trueds, is a a king,\n",
      "Lo velence brooking hear her to maws, sut's a samy for proen!\n",
      "To as liendre, poon not here, letce\n",
      "And the must I inot reight to verfishshil\n",
      "26 1.7713858618055072\n",
      "The King said,\n",
      "Be to-morchinned?\n",
      "-Naaky not the wanriet be the exthou thee,\n",
      "With a welr bopve tingers lake hand, but our with my bear.\n",
      "\n",
      "SATENSI:\n",
      "No sup hitbill prosk you?\n",
      "\n",
      "Ds Yit:\n",
      "But ingin this will whom thy cume\n",
      "\n",
      "27 1.763982423373631\n",
      "The King said: whom froth some:\n",
      "And all your or not oo lister; that a sing\n",
      "Are wolsing gue can wise be his.\n",
      "\n",
      "Firns thour:\n",
      "Whosaver ear and him be I so kippinter?\n",
      "\n",
      "Shcomp: ancy:\n",
      "Haed shefate of the tires.\n",
      "\n",
      "PEMCALENI\n",
      "28 1.75692741053445\n",
      "The King said.\n",
      "\n",
      "TRANIO:\n",
      "WaA deatipe die of her-more, guon; you have mord\n",
      "Nors is doth ouf be light gat, that should foor courthay,\n",
      "Have prices be your him feth hearte ofndact\n",
      "At wonder wath his way, which, thou, ag\n",
      "29 1.7507111726488387\n",
      "The King said.\n",
      "\n",
      "MERCUTIO:\n",
      "Theres Rome Bomi comving undight mashest's all feet.\n",
      "Ithen swrand his nost yamnenst, frient\n",
      "Oo this hear ban home, but, if he being affear,\n",
      "For agay of old more coulsuincipfonds, and on Su\n",
      "30 1.7439696611676898\n",
      "The King said not hand:\n",
      "O but when not inmfoed and your samy his dant\n",
      "to, be of a find give-folder.\n",
      "\n",
      "LOMEO:\n",
      "Ay safce to thee enviled,\n",
      "Meelguning on air; and mernow them, so if the had agien If\n",
      "For noting on think-s\n",
      "31 1.7386828906195504\n",
      "The King said to firdher't me,\n",
      "Id be parder of morn-shently spirts me,\n",
      "I willl; will of Richister, the cander, and Celasket with\n",
      "I great so be eviching alvipe I pnowing\n",
      "Herant of I no on slark,\n",
      "the hath shall best \n",
      "32 1.7323625714438302\n",
      "The King said\n",
      "Sind!\n",
      "\n",
      "KING EDWARD IV:\n",
      "God bane turs I woof.\n",
      "\n",
      "ANTONIO:\n",
      "Murder slits Froul skop the'll my singland have be great to graag\n",
      "Of to this rize, what eath of othinful\n",
      "Anwent, and me hore as you deaft;\n",
      "As sis\n",
      "33 1.727274239403861\n",
      "The King said onem, I canns to and your of with Clivirip'd they berot thet an the\n",
      "do Do dale by hus art, ancerfery:\n",
      "And suntle of more may I pexasite.\n",
      "\n",
      "RICKANS:\n",
      "Fors one's this partagun is but shap,\n",
      "Leed no stright\n",
      "34 1.7218051699229648\n",
      "The King saidaron to do when,\n",
      "He train of our to Lothar I gowher,\n",
      "And, all the home in the lows say, lovem'd hand;\n",
      "Ifjy bechard's wo'ath comes, so leave's\n",
      "And the hath lives'd him to toogh she'st I'll how,\n",
      "And my t\n",
      "35 1.717290688923427\n",
      "The King saids thy be stears.\n",
      "\n",
      "Nurts:\n",
      "I hath falfod it him, therendself's prince,\n",
      "With my tongry, which it as I cants,--\n",
      "\n",
      "Shwaivan:\n",
      "A fors our Canaip, that a his all not he side,\n",
      "To couppacesting a reperp to the re\n",
      "36 1.7123376635142735\n",
      "The King saidapent,\n",
      "Hate fere open me have be hears wood worded\n",
      "On the snown Kon, I slack; the witly?\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Tawer's greed's all keaced York I have: not was.\n",
      "\n",
      "TRANIO:\n",
      "Go mistram, and liiteal acreade us f\n",
      "37 1.7075202648980277\n",
      "The King said of regpattaw's nose.\n",
      "\n",
      "GRUMIO:\n",
      "Ol I would feon prociou\n",
      "ir, and theister:\n",
      "And my canot end foret the chore frot now,\n",
      "'Till, let resquart insfence. My last and chark.\n",
      "\n",
      "QUEEN ROREm:\n",
      "Good do princed oled m\n",
      "38 1.70341037273407\n",
      "The King saidsly, fare'tle have nuse his orriatn,\n",
      "Nor hear evice; I gricrard Cart, as dement!\n",
      "\n",
      "LUCIO:\n",
      "I do a pray: as my but, is head\n",
      "Coright me enwletsing, the fait'd we if remett me;\n",
      "Yree you no ewder seld king K\n",
      "39 1.6991004712241036\n",
      "The King saids, divarry rond?\n",
      "The grase in unto take to I untrightune.\n",
      "\n",
      "MENENIUS:\n",
      "Seclapery and now, at all a what sermch upon?\n",
      "Be's Alady grag lemenes to us b.\n",
      "\n",
      "FRIAR AN:\n",
      "Sirrel lovn to these his shome:\n",
      "Lhow he wo\n",
      "40 1.6954754883902414\n",
      "The King saids and knot resing discitact:\n",
      "That togell,--Sleay mad that my mubidue, sir:\n",
      "I pervarl's fror gotter blood if but the sold\n",
      "By neven. Ffan look abvow of that grens to fast\n",
      "Bedan, Wild engart of and wall o\n",
      "41 1.691845383644104\n",
      "The King said's Lordte trien.\n",
      "\n",
      "KING EDWARD IV:\n",
      "I all him'n a with more afford cane,\n",
      "Doke and besing of I golb,\n",
      "Of show o't thee state?\n",
      "\n",
      "CAPULET:\n",
      "Proper as Rilost, and the knee indaul'd,\n",
      "Here fore, and undented: you\n",
      "42 1.6879461996895926\n",
      "The King said, a for wall eyee, out:\n",
      "And for most stree the viity evate,\n",
      "And all my land.\n",
      "\n",
      "TIRCHARD:\n",
      "No, sir, go didst thy have her distrusce mebong.\n",
      "I thee!\n",
      "\n",
      "ESCALUS:\n",
      "O Norstruth that her unwert:\n",
      "The soventy but b\n",
      "43 1.6846432713099888\n",
      "The King said navess do.\n",
      "Fayhell!\n",
      "\n",
      "BRUTUS:\n",
      "I shair well, goat love me their cuse it partion:\n",
      "Sun this Hompures to his wither parsullecs;\n",
      "Day, with basts love's si knowfied my pright.\n",
      "\n",
      "Fir: IDAUPO:\n",
      "A comars, youth d\n",
      "44 1.6817947122028896\n",
      "The King saids An Sicilius,\n",
      "Us my beac be same of will it:\n",
      "The trance soleth's hears's fairs, o't to them.\n",
      "The could too up I stroppor with that part play thyer.\n",
      "Says made the word is haves to do should couded\n",
      "By I\n",
      "45 1.677480503490993\n",
      "The King saidforstilvent endlang.\n",
      "\n",
      "Mestreatn:\n",
      "Rould, loked that your; Sent on, the tust see to to joir ad\n",
      "To can lon to come it. Sir; fitter you.\n",
      "\n",
      "KING HENRY OF ANE:\n",
      "'Cvidy for this frience, the come, wan henced\n",
      "To\n",
      "46 1.6746438775743757\n",
      "The King saidanesty bade loce, I am they me\n",
      "Strabere the would me conmore,\n",
      "And Walke flest tick your life.\n",
      "\n",
      "Second Wastalt:\n",
      "The peace for be should as with good to.\n",
      "Should that age! and with the pircher'd make sorr\n",
      "47 1.6720981482097081\n",
      "The King said as sovereash in nible\n",
      "Preads, and bauch distress-dowh noted pleow, liel hazter;\n",
      "Their furse should.\n",
      "\n",
      "JULIET:\n",
      "He,\n",
      "Ir sages I faish; to like theur honour\n",
      "Thou nobburgunly deat go with go,\n",
      "I? You fear, t\n",
      "48 1.6691185644694737\n",
      "The King saids of but your honewel'd with\n",
      "But your of go to thee, gries your after.\n",
      "\n",
      "RATHEYN:\n",
      "And fie to Ane, to him thime, in you stand craif.\n",
      "\n",
      "MENENIUS:\n",
      "My Sary; and sensear. For my mark my cause one-foroul,\n",
      "That\n",
      "49 1.6658464683805194\n",
      "The King said 'twas!\n",
      "Thou some my not? Herefore.\n",
      "\n",
      "GRUMIO:\n",
      "\n",
      "AUTOLYCUS:\n",
      "He horsileting good that some us gried.\n",
      "\n",
      "KING RICHARD III:\n",
      "Bestly from poward clieges is all causysed.\n",
      "\n",
      "AUTOLYFCRA:\n",
      "God-dies in make we diebless\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "net = SequenceGenerationNet(vocab_size, 20, 50, num_layers=2, dropout=0.1)\n",
    "opt = optim.Adam(net.parameters())\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "for epoch in range(50):\n",
    "    net.train()\n",
    "    losses = []\n",
    "    for data in loader:\n",
    "        x = V(data[:, :-1])\n",
    "        y = V(data[:, 1:])\n",
    "        y_pred, _ = net(x)\n",
    "        # https://discuss.pytorch.org/t/runtimeerror-input-is-not-contiguous/930/8\n",
    "        loss = loss_f(y_pred.view(-1, vocab_size), y.contiguous().view(-1))\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.data[0])\n",
    "    print('=================================================================================')\n",
    "    print(epoch, mean(losses))\n",
    "    print(generate_seq(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "\n",
    "# http://www.manythings.org/anki/spa-eng.zip\n",
    "remove_marks_regex = re.compile('[,\\.\\(\\)\\[\\]\\*:;¿¡]|<.*?>')\n",
    "shift_marks_regex = re.compile('([?!\\.])')\n",
    "unk = 0\n",
    "sos = 1\n",
    "eos = 2\n",
    "\n",
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    text = remove_marks_regex.sub('', text)\n",
    "    text = shift_marks_regex.sub(r' \\1', text)\n",
    "    return text\n",
    "\n",
    "def parse_line(line):\n",
    "    line = normalize(line.strip())\n",
    "    src, trg = line.split('\\t')\n",
    "    src_tokens = src.strip().split()\n",
    "    trg_tokens = trg.strip().split()\n",
    "    return src_tokens, trg_tokens\n",
    "\n",
    "def build_vocab(tokens):\n",
    "    counts = collections.Counter(tokens)\n",
    "    sorted_counts = sorted(counts.items(), key=lambda c: c[1], reverse=True)\n",
    "    word_list = ['<UNK>', '<SOS>', '<EOS>'] + [x[0] for x in sorted_counts]\n",
    "    word_dict = dict((w, i) for i, w in enumerate(word_list))\n",
    "    return word_list, word_dict\n",
    "\n",
    "def words2tensor(words, word_dict, max_len, padding=0):\n",
    "    words = words + ['<EOS>']\n",
    "    words = [word_dict.get(w, 0) for w in words]\n",
    "    seq_len = len(words)\n",
    "    if seq_len < max_len + 1:\n",
    "        words = words + [padding] * (max_len + 1 - seq_len)\n",
    "    return torch.LongTensor(words), seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationPairDataset(Dataset):\n",
    "    def __init__(self, path, max_len=15):\n",
    "        def filter_pair(p):\n",
    "            return not (len(p[0]) > max_len or len(p[1]) > max_len)\n",
    "        \n",
    "        with open(path) as fp:\n",
    "            pairs = map(parse_line, fp)\n",
    "            pairs = filter(filter_pair, pairs)\n",
    "            pairs = list(pairs)\n",
    "        src = [p[0] for p in pairs]\n",
    "        trg = [p[1] for p in pairs]\n",
    "        self.src_word_list, self.src_word_dict = build_vocab(itertools.chain.from_iterable(src))\n",
    "        self.trg_word_list, self.trg_word_dict = build_vocab(itertools.chain.from_iterable(trg))\n",
    "        self.src_data = [words2tensor(words, self.src_word_dict, max_len) for words in src]\n",
    "        self.trg_data = [words2tensor(words, self.trg_word_dict, max_len, -100) for words in trg]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src, lsrc = self.src_data[idx]\n",
    "        trg, ltrg = self.trg_data[idx]\n",
    "        return src, lsrc, trg, ltrg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_len = 10\n",
    "path = './spa.txt'\n",
    "ds = TranslationPairDataset(path, max_len=max_len)\n",
    "loader = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim=50, hidden_size=50, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, h0=None, l=None):\n",
    "        x = self.emb(x)\n",
    "        if l is not None:\n",
    "            x = nn.utils.rnn.pack_padded_sequence(x, l, batch_first=True)\n",
    "        _, h = self.lstm(x, h0)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim=50, hidden_size=50, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, num_embeddings)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        x = self.emb(x)\n",
    "        x, h = self.lstm(x, h)\n",
    "        x = x.view(-1, self.lstm.hidden_size)\n",
    "        x = self.linear(x)\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_str, enc, dec, max_len=15):\n",
    "    words = normalize(input_str).split()\n",
    "    input_tensor, seq_len = words2tensor(words, ds.src_word_dict, max_len=max_len)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "    seq_len = [seq_len]\n",
    "    sos_inputs = torch.LongTensor([sos]).unsqueeze(1)\n",
    "    ctx = enc(V(input_tensor, volatile=True), l=seq_len)\n",
    "    z = V(sos_inputs, volatile=True)\n",
    "    h = ctx\n",
    "    results = []\n",
    "    for i in range(max_len):\n",
    "        o, h = dec(z, h)\n",
    "        wi = o.data.max(1)[1].view(1)\n",
    "        if wi[0] == eos:\n",
    "            break\n",
    "        results.append(wi[0])\n",
    "        z = V(wi.view(1, 1), volatile=True)\n",
    "    return \" \".join(ds.trg_word_list[i] for i in results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'monja lavásemos lavásemos lavásemos lavásemos cuarentas preso residentes residentes reventado reventado prius vivir vivir reventado'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder(len(ds.src_word_list), 100, 100, 2)\n",
    "dec = Decoder(len(ds.trg_word_list), 100, 100, 2)\n",
    "translate('I am a student.', enc, dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(len(ds.src_word_list), 100, 100, 1, dropout=0.1)\n",
    "dec = Decoder(len(ds.trg_word_list), 100, 100, 1, dropout=0.1)\n",
    "opt_enc = optim.Adam(enc.parameters(), 0.002)\n",
    "opt_dec = optim.Adam(dec.parameters(), 0.01)\n",
    "loss_f = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 48.09250288635291\n",
      "soy un\n",
      "a le gusta a a\n",
      "ella es mi padre\n",
      "1 38.03870529137383\n",
      "soy un\n",
      "a gusta gusta a a\n",
      "ella es mi madre madre\n",
      "2 32.706015105630485\n",
      "soy un estudiante\n",
      "a gusta le gusta pizza pizza\n",
      "ella es mi madre\n",
      "3 29.407002432078837\n",
      "soy un estudiante\n",
      "a gustan comer pizza pizza\n",
      "ella es mi\n",
      "4 27.15345342578806\n",
      "soy un estudiante\n",
      "a gusta le gustan\n",
      "ella es mi\n",
      "5 25.54423598377782\n",
      "soy estudiante\n",
      "le gusta pizza pizza pizza pizza\n",
      "ella es mi madre\n",
      "6 24.301904748510832\n",
      "soy un estudiante\n",
      "a gusta le pizza pizza pizza\n",
      "ella es mi madre\n",
      "7 23.38268745464463\n",
      "soy un estudiante\n",
      "a gusta le gusta pizza pizza\n",
      "ella es mi madre\n",
      "8 22.60478694361459\n",
      "soy estudiante estudiante\n",
      "a gusta quiere pizza pizza\n",
      "ella es mi madre\n",
      "9 22.026191638336087\n",
      "soy estudiante\n",
      "a gusta comer pizza pizza\n",
      "ella es mi madre\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "\n",
    "for epoc in range(10):\n",
    "    enc.train()\n",
    "    dec.train()\n",
    "    losses = []\n",
    "    for x, lx, y, ly in loader:\n",
    "        sos_inputs = torch.LongTensor([sos] * len(x)).unsqueeze(1)\n",
    "        lx, sort_idx = lx.sort(descending=True)\n",
    "        x, y = x[sort_idx], y[sort_idx]\n",
    "        x, y = V(x), V(y)\n",
    "        loss = 0\n",
    "        ctx = enc(x, l=list(lx))\n",
    "        z = V(sos_inputs)\n",
    "        h = ctx\n",
    "        for i in range(max_len):\n",
    "            o, h = dec(z, h)\n",
    "            loss += loss_f(o, y[:, i])\n",
    "            wi = o.data.max(1)[1].unsqueeze(1)\n",
    "            z = V(wi)\n",
    "        enc.zero_grad()\n",
    "        dec.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_enc.step()\n",
    "        opt_dec.step()\n",
    "        losses.append(loss.data[0])\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    print('===================================================================================')\n",
    "    print(epoc, mean(losses))\n",
    "    print(translate('I am a student.', enc, dec, max_len=max_len))\n",
    "    print(translate('He likes to eat pizza.', enc, dec, max_len=max_len))\n",
    "    print(translate('She is my mother.', enc, dec, max_len=max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
